{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and move BenthicNet data over to local node storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "move_script_path = \"./slurm/copy_and_extract_data.sh\"\n",
    "subprocess.run([\"bash\", move_script_path], check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display system GPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_available_gpus():\n",
    "    \"\"\"Get a list of available GPUs on the system.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        gpu_names = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
    "        return gpu_names\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "available_gpus = get_available_gpus()\n",
    "if available_gpus:\n",
    "    print(\"Available GPUs:\")\n",
    "    for i, gpu in enumerate(available_gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs available on the system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set arguments and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"hp_imagenet_v2_rn50\"\n",
    "train_cfg_path = \"./cfgs/cnn/resnet50_hl.json\"\n",
    "graph_path = \"./graph_info/finalized_output.csv\"\n",
    "model_checkpoint = f\"./pretrained_encoders/{model}.ckpt\"\n",
    "data_csv_path = \"./data_csv/benthicnet_nn.csv\"\n",
    "tar_dir = \"/gpfs/project/6012565/become_labelled/compiled_labelled_512px/tar\"\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "test_head = \"substrate\"\n",
    "\n",
    "test_random = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define graph related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "_HEAD_TO_COLUMN_MAP = {\n",
    "    \"biota\": \"CATAMI Biota\",\n",
    "    \"substrate\": \"CATAMI Substrate\",\n",
    "    \"relief\": \"CATAMI Relief\",\n",
    "    \"bedforms\": \"CATAMI Bedforms\",\n",
    "}\n",
    "\n",
    "_HEAD_TO_MASK_MAP = {\n",
    "    \"biota\": \"Biota Mask\",\n",
    "    \"substrate\": \"Substrate Mask\",\n",
    "    \"relief\": \"Relief Mask\",\n",
    "    \"bedforms\": \"Bedforms Mask\",\n",
    "}\n",
    "\n",
    "\n",
    "# Utilities for graph functions\n",
    "def check_elements(list_a, list_b):\n",
    "    return any(element in set(list_b) for element in set(list_a))\n",
    "\n",
    "\n",
    "# General graph related functions\n",
    "def create_subgraph(G, radius):\n",
    "    G = G.copy()\n",
    "\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    root_nodes = [node for node in nx.topological_sort(G) if G.in_degree(node) == 0]\n",
    "\n",
    "    pruned_G = nx.DiGraph()\n",
    "\n",
    "    nodes_within_radius = []\n",
    "    nodes_exact_radius = []\n",
    "    for start_node in root_nodes:\n",
    "        sub_G = nx.ego_graph(G=G, n=start_node, radius=radius)\n",
    "\n",
    "        nodes_and_distances = nx.single_source_shortest_path_length(G, start_node)\n",
    "\n",
    "        # nodes within radius\n",
    "        nodes_within_radius_sub = [\n",
    "            node for node, distance in nodes_and_distances.items() if distance <= radius\n",
    "        ]\n",
    "\n",
    "        # nodes with exact radius (include start_node)\n",
    "        nodes_exact_radius_sub = [\n",
    "            node for node, distance in nodes_and_distances.items() if distance == radius\n",
    "        ]\n",
    "\n",
    "        # updates\n",
    "        nodes_within_radius += nodes_within_radius_sub\n",
    "        nodes_exact_radius += nodes_exact_radius_sub\n",
    "        pruned_G.update(sub_G)\n",
    "\n",
    "    return pruned_G, nodes_within_radius, nodes_exact_radius\n",
    "\n",
    "\n",
    "def filter_data(df, head, depth, leaf_nodes, column):\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[column])\n",
    "\n",
    "    df.loc[:, column] = df.loc[:, column].fillna(\"\")\n",
    "    df.loc[:, f\"_{head}\"] = df.loc[:, column].apply(\n",
    "        lambda x: np.where(np.array(x) == 1)[0]\n",
    "    )\n",
    "    df.loc[:, f\"{head}_depth_{depth}_applicable\"] = df.loc[:, f\"_{head}\"].apply(\n",
    "        check_elements, args=(leaf_nodes,)\n",
    "    )\n",
    "    df.drop([f\"_{head}\"], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_data_with_hops(df, hops_dict, root_graphs, column):\n",
    "    for head, hop in hops_dict.items():\n",
    "        root_graph = root_graphs[head]\n",
    "        _, nodes_within_radius, nodes_exact_radius = create_subgraph(root_graph, hop)\n",
    "        df = filter_data(df, head, hop, nodes_exact_radius, column)\n",
    "    return df, nodes_within_radius\n",
    "\n",
    "\n",
    "def find_max_depth(graph):\n",
    "    max_depth = 0\n",
    "\n",
    "    graph_nodes = set(graph.nodes())\n",
    "    nodes_within_max_depth = set()\n",
    "\n",
    "    while not graph_nodes.issubset(nodes_within_max_depth):\n",
    "        _, nodes_within_radius, _ = create_subgraph(graph, max_depth)\n",
    "        nodes_within_max_depth = set(nodes_within_radius)\n",
    "        max_depth += 1\n",
    "\n",
    "    return max_depth - 1\n",
    "\n",
    "\n",
    "def remove_relevant_masked_nodes(mask, relevant_nodes):\n",
    "    return np.array([node for node in relevant_nodes if node not in set(mask)])\n",
    "\n",
    "\n",
    "def extract_relevant_elements(row, col, idx_col):\n",
    "    return row[col][row[idx_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import benthicnet dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import PIL\n",
    "\n",
    "from utils.benthicnet_dataset import BenthicNetDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from utils.utils import construct_model, gen_R_mat, gen_root_graphs\n",
    "\n",
    "with open(train_cfg_path, \"r\") as f:\n",
    "    train_cfg_content = f.read()\n",
    "\n",
    "train_cfg = loads(train_cfg_content)\n",
    "train_kwargs = OmegaConf.create(train_cfg)\n",
    "\n",
    "# Get graphs and adjacency matrices\n",
    "root_graphs, idx_to_node, node_to_idx = gen_root_graphs(graph_path)\n",
    "Rs = {root: gen_R_mat(graph) for root, graph in root_graphs.items()}\n",
    "\n",
    "# Build model\n",
    "model = construct_model(train_kwargs, Rs, model_checkpoint, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.utils import get_augs, get_df, process_data_df\n",
    "\n",
    "df = get_df(data_csv_path)\n",
    "df = df[~pd.isna(df[_HEAD_TO_COLUMN_MAP[test_head]])]\n",
    "\n",
    "test_df = df[df[\"partition\"] == \"test\"]\n",
    "\n",
    "_, val_transform = get_augs(False)\n",
    "\n",
    "test_df = process_data_df(test_df, Rs)\n",
    "\n",
    "test_dataset = BenthicNetDataset(\n",
    "    tar_dir=tar_dir, annotations=test_df, transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test loop to get model output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model.eval()\n",
    "\n",
    "preds = []\n",
    "tgts = []\n",
    "masks = []\n",
    "prev_pred = 0\n",
    "\n",
    "total_batches = len(test_dataloader)\n",
    "# Test data loop\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    print(f\"Processing batch {i+1}/{total_batches}...\", end=\"\\r\")\n",
    "\n",
    "    # New seed every loop\n",
    "    torch.manual_seed(i)\n",
    "\n",
    "    # Move items to device\n",
    "    inputs, data = batch\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    batch = [inputs, data]\n",
    "    batch_preds, batch_tgts, batch_masks = model.predict_head_step(\n",
    "        batch, head=test_head, random_out=test_random\n",
    "    )\n",
    "\n",
    "    assert len(batch_preds) == len(batch_tgts)\n",
    "    assert len(batch_tgts) == len(batch_masks)\n",
    "    preds.extend(batch_preds.tolist())\n",
    "    tgts.extend(batch_tgts.tolist())\n",
    "    masks.extend(batch_masks.tolist())\n",
    "\n",
    "model_output_dict = {\n",
    "    \"preds\": preds,\n",
    "    \"tgts\": tgts,\n",
    "    \"masks\": masks,\n",
    "}\n",
    "\n",
    "model_output_df = pd.DataFrame(model_output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_df_correct(row):\n",
    "    assert len(row[\"relevant_preds\"]) == len(row[\"relevant_tgts\"])\n",
    "    return all(row[\"relevant_preds\"] == row[\"relevant_tgts\"])\n",
    "\n",
    "\n",
    "def depth_df_avg_bit_f1(row):\n",
    "    pred = row[\"relevant_preds\"]\n",
    "    tgt = row[\"relevant_tgts\"]\n",
    "    assert len(pred) == len(tgt)\n",
    "\n",
    "    avg_f1 = np.zeros(2)\n",
    "    for state in [0, 1]:\n",
    "        # These are the bits that tgt says is equal to \"state\"\n",
    "        state_mask = tgt == state\n",
    "\n",
    "        # What pred says for the bits obtained above\n",
    "        tp = np.sum(pred[state_mask] == state)\n",
    "        fn = np.sum(pred[state_mask] != state)\n",
    "        fp = np.sum(pred[~state_mask] == state)\n",
    "        tn = np.sum(pred[~state_mask] != state)\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "        if (precision + recall) == 0:\n",
    "            avg_f1[state] = 0\n",
    "        else:\n",
    "            avg_f1[state] = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return np.mean(np.array(avg_f1))\n",
    "\n",
    "\n",
    "def depth_df_bit_mcc(row):\n",
    "    pred = row[\"relevant_preds\"]\n",
    "    tgt = row[\"relevant_tgts\"]\n",
    "    assert len(pred) == len(tgt)\n",
    "\n",
    "    state = 0\n",
    "    state_mask = tgt == state\n",
    "\n",
    "    # What pred says for the bits obtained above\n",
    "    tp = np.sum(pred[state_mask] == state)\n",
    "    fn = np.sum(pred[state_mask] != state)\n",
    "    fp = np.sum(pred[~state_mask] == state)\n",
    "    tn = np.sum(pred[~state_mask] != state)\n",
    "\n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    if denominator == 0:\n",
    "        mcc = 0\n",
    "    else:\n",
    "        mcc = numerator / denominator\n",
    "\n",
    "    return mcc\n",
    "\n",
    "\n",
    "def set_depth_idx_to_text_map(relevant_nodes, idx_to_text_maps, head):\n",
    "    idx_to_text_map = idx_to_text_maps[head]\n",
    "\n",
    "    depth_map = {i: idx_to_text_map[idx] for i, idx in enumerate(relevant_nodes)}\n",
    "\n",
    "    return depth_map\n",
    "\n",
    "\n",
    "# Define translation functions\n",
    "def translate_idx_to_text(idx_array, idx_to_text_map, head):\n",
    "    if isinstance(idx_array, str):\n",
    "        idx_array = ast.literal_eval(idx_array)\n",
    "    text_array = [idx_to_text_map[idx] for idx in idx_array]\n",
    "    unique_texts = set(text_array)\n",
    "    text_result = []\n",
    "\n",
    "    for text in text_array:\n",
    "        if not any(text in item for item in unique_texts if item != text):\n",
    "            text_result.append(text)\n",
    "\n",
    "    return text_result\n",
    "\n",
    "\n",
    "def translate_idx_to_text_row(row, col_to_translate, map_column, head):\n",
    "    idx_to_text_map = row[map_column]\n",
    "    idx_array = row[col_to_translate]\n",
    "    if isinstance(idx_array, str):\n",
    "        idx_array = ast.literal_eval(idx_array)\n",
    "    text_array = [idx_to_text_map[idx] for idx in idx_array]\n",
    "    unique_texts = set(text_array)\n",
    "    text_result = []\n",
    "\n",
    "    for text in text_array:\n",
    "        if not any(text in item for item in unique_texts if item != text):\n",
    "            text_result.append(text)\n",
    "\n",
    "    return text_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main processing loop for depth metrics for fused/superposition labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graph(G, size, color):\n",
    "    options = {\n",
    "        \"node_size\": size,\n",
    "        \"node_color\": color,\n",
    "        # \"alpha\": 0.6,\n",
    "        \"with_labels\": True,\n",
    "    }\n",
    "\n",
    "    nx.draw(G, pos=nx.spring_layout(G, seed=42), **options)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = find_max_depth(root_graphs[test_head])\n",
    "\n",
    "depth_dict = {\n",
    "    test_head: 0,\n",
    "}\n",
    "\n",
    "output_df = model_output_df.copy()\n",
    "\n",
    "accs = np.zeros(max_depth + 1)\n",
    "f1s = np.zeros(max_depth + 1)\n",
    "mccs = np.zeros(max_depth + 1)\n",
    "sample_sizes = np.zeros(max_depth + 1)\n",
    "\n",
    "for depth in range(max_depth + 1):\n",
    "    # Set depth-wise column names\n",
    "    relevant_nodes_col = f\"relevant_{test_head}_nodes_at_depth_{depth}\"\n",
    "    relevant_map_col = f\"idx_to_node_{test_head}_at_depth_{depth}\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Processing depth {depth} of {max_depth} for {test_head}...\")\n",
    "    depth_dict[test_head] = depth\n",
    "    depth_df, nodes_within_radius = filter_data_with_hops(\n",
    "        output_df, depth_dict, root_graphs, column=\"tgts\"\n",
    "    )\n",
    "    depth_df = depth_df[depth_df[f\"{test_head}_depth_{depth}_applicable\"] == True]\n",
    "    depth_df[\"_masks\"] = depth_df[\"masks\"].apply(\n",
    "        lambda x: np.where(np.array(x) == 0)[0]\n",
    "    )\n",
    "\n",
    "    # Determine which nodes are relevant at current depth after filtering for masks\n",
    "    depth_df[relevant_nodes_col] = depth_df[\"_masks\"].apply(\n",
    "        remove_relevant_masked_nodes, args=(nodes_within_radius,)\n",
    "    )\n",
    "    depth_df[relevant_map_col] = depth_df[relevant_nodes_col].apply(\n",
    "        set_depth_idx_to_text_map, args=(idx_to_node, test_head)\n",
    "    )\n",
    "    depth_df[\"preds\"] = depth_df[\"preds\"].apply(lambda x: np.array(x))\n",
    "    depth_df[\"tgts\"] = depth_df[\"tgts\"].apply(lambda x: np.array(x))\n",
    "\n",
    "    # Filter preds and tgts for relevant nodes\n",
    "    depth_df[\"relevant_preds\"] = depth_df.apply(\n",
    "        extract_relevant_elements, args=(\"preds\", relevant_nodes_col), axis=1\n",
    "    )\n",
    "    depth_df[\"relevant_tgts\"] = depth_df.apply(\n",
    "        extract_relevant_elements, args=(\"tgts\", relevant_nodes_col), axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate acc and bit_f1\n",
    "    depth_df[\"correct\"] = depth_df.apply(depth_df_correct, axis=1)\n",
    "    depth_df[\"avg_bit_f1\"] = depth_df.apply(depth_df_avg_bit_f1, axis=1)\n",
    "    depth_df[\"bit_mcc\"] = depth_df.apply(depth_df_bit_mcc, axis=1)\n",
    "\n",
    "    # Have to convert tgts to string to be hashable\n",
    "    depth_df[\"relevant_tgts_str\"] = depth_df[\"relevant_tgts\"].apply(\n",
    "        lambda x: str(list(np.where(x == 1)[0]))\n",
    "    )\n",
    "    depth_df[\"relevant_preds_str\"] = depth_df[\"relevant_preds\"].apply(\n",
    "        lambda x: str(list(np.where(x == 1)[0]))\n",
    "    )\n",
    "    depth_acc = []\n",
    "    depth_bit_f1 = []\n",
    "    depth_bit_mcc = []\n",
    "\n",
    "    unique_labels = depth_df[\"relevant_tgts_str\"].unique()\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_df = depth_df[depth_df[\"relevant_tgts_str\"] == label].copy()\n",
    "        label_acc = np.sum(label_df[\"correct\"]) / len(label_df)\n",
    "        label_bit_f1 = np.sum(label_df[\"avg_bit_f1\"]) / len(label_df)\n",
    "        label_bit_mcc = np.sum(label_df[\"bit_mcc\"]) / len(label_df)\n",
    "\n",
    "        # Determine most common prediction from our model\n",
    "        most_common_pred = label_df[\"relevant_preds_str\"].value_counts().idxmax()\n",
    "\n",
    "        # Determine how frequently most common prediction is made\n",
    "        most_common_df = label_df[label_df[\"relevant_preds_str\"] == most_common_pred]\n",
    "        num_most_common = len(\n",
    "            label_df[label_df[\"relevant_preds_str\"] == most_common_pred]\n",
    "        )\n",
    "        num_samples_label = len(label_df)\n",
    "        most_common_pct = num_most_common / num_samples_label\n",
    "\n",
    "        # Determine relevant map\n",
    "        idx_map = (\n",
    "            label_df.loc[label_df[\"relevant_preds_str\"] == most_common_pred, :][\n",
    "                relevant_map_col\n",
    "            ]\n",
    "            .copy()\n",
    "            .reset_index(drop=True)[0]\n",
    "        )\n",
    "\n",
    "        most_common_pred = translate_idx_to_text(\n",
    "            most_common_pred, idx_map, head=test_head\n",
    "        )\n",
    "\n",
    "        label_text = translate_idx_to_text(label, idx_map, head=test_head)\n",
    "        print(\"\\n\\t\" + \"-\" * 40)\n",
    "        # print(f\"\\tRaw Label: {label}\")\n",
    "        print(f\"\\tLabel: {label_text}\")\n",
    "        print(\"\\tAcc:\", label_acc)\n",
    "        print(\"\\tF1:\", label_bit_f1)\n",
    "        print(\"\\tMCC:\", label_bit_mcc)\n",
    "        print(\"\\tMost common pred:\", most_common_pred)\n",
    "        print(\"\\tPredicted frequency:\", most_common_pct)\n",
    "        print(\"\\tNumber of samples for label:\", num_samples_label)\n",
    "\n",
    "        depth_acc.append(label_acc)\n",
    "        depth_bit_f1.append(label_bit_f1)\n",
    "        depth_bit_mcc.append(label_bit_mcc)\n",
    "\n",
    "    mean_depth_acc = np.mean(depth_acc)\n",
    "    mean_depth_bit_f1 = np.mean(depth_bit_f1)\n",
    "    mean_depth_bit_mcc = np.mean(depth_bit_mcc)\n",
    "    sample_count = len(depth_df)\n",
    "    print(\"\\n\\t\" + \"-\" * 60)\n",
    "    print(f\"\\tDepth {depth} acc: {mean_depth_acc}\")\n",
    "    print(f\"\\tDepth {depth} bit F1: {mean_depth_bit_f1}\")\n",
    "    print(f\"\\tDepth {depth} bit MCC: {mean_depth_bit_mcc}\")\n",
    "    print(f\"\\tDepth {depth} sample count: {sample_count}\")\n",
    "\n",
    "    accs[depth] = mean_depth_acc\n",
    "    f1s[depth] = mean_depth_bit_f1\n",
    "    mccs[depth] = mean_depth_bit_mcc\n",
    "    sample_sizes[depth] = sample_count\n",
    "\n",
    "head_acc = np.sum(accs * sample_sizes) / np.sum(sample_sizes)\n",
    "macro_head_acc = np.mean(accs)\n",
    "head_f1 = np.sum(f1s * sample_sizes) / np.sum(sample_sizes)\n",
    "head_mcc = np.sum(mccs * sample_sizes) / np.sum(sample_sizes)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Sample weighted accuracy for {test_head}: {head_acc}\")\n",
    "print(f\"Macro accuracy for {test_head}: {macro_head_acc}\")\n",
    "print(f\"Bit F1 for {test_head}: {head_f1}\")\n",
    "print(f\"Bit MCC for {test_head}: {head_mcc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to decompose labels and score performance on individual targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_and_score_labels(\n",
    "    row, tp_dict, fp_dict, fn_dict, count_dict, depth, verbose=0\n",
    "):\n",
    "    tgts = row[\"text_tgts\"]\n",
    "    tgts_len = len(tgts)\n",
    "\n",
    "    preds = set(row[\"text_preds\"])\n",
    "    original_preds_len = len(preds)\n",
    "\n",
    "    if verbose > 0 and original_preds_len > tgts_len:\n",
    "        print(f\"\\tExtra predictions detected for {tgts}.\")\n",
    "        print(f\"Targets: {tgts}\")\n",
    "        print(f\"Predictions: {preds}\")\n",
    "\n",
    "    local_tp_dict = {}\n",
    "    local_fp_dict = {}\n",
    "    local_fn_dict = {}\n",
    "    local_count_dict = {}\n",
    "\n",
    "    for tgt in tgts:\n",
    "        split_symbol_count = tgt.count(\" > \")\n",
    "        if split_symbol_count == depth:\n",
    "            local_count_dict[tgt] = 1\n",
    "            if tgt in preds:\n",
    "                local_tp_dict[tgt] = 1\n",
    "                local_fp_dict[tgt] = 0\n",
    "                local_fn_dict[tgt] = 0\n",
    "                preds.remove(tgt)\n",
    "            else:\n",
    "                local_tp_dict[tgt] = 0\n",
    "                local_fp_dict[tgt] = 0\n",
    "                local_fn_dict[tgt] = 1\n",
    "\n",
    "    for pred in preds:\n",
    "        split_symbol_count = pred.count(\" > \")\n",
    "        if split_symbol_count == depth:\n",
    "            if pred not in tgts:\n",
    "                local_tp_dict[pred] = 0\n",
    "                local_fp_dict[pred] = 1\n",
    "                local_fn_dict[pred] = 0\n",
    "\n",
    "    for label in local_tp_dict.keys():\n",
    "        tp_dict[label] = tp_dict.get(label, 0) + local_tp_dict[label]\n",
    "        fp_dict[label] = fp_dict.get(label, 0) + local_fp_dict.get(label, 0)\n",
    "        fn_dict[label] = fn_dict.get(label, 0) + local_fn_dict.get(label, 0)\n",
    "        count_dict[label] = count_dict.get(label, 0) + local_count_dict.get(label, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main processing loop for calculating decomposed label accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = find_max_depth(root_graphs[test_head])\n",
    "\n",
    "depth_dict = {\n",
    "    test_head: 0,\n",
    "}\n",
    "\n",
    "output_df = model_output_df.copy()\n",
    "\n",
    "precision = np.zeros(max_depth + 1)\n",
    "recall = np.zeros(max_depth + 1)\n",
    "f1 = np.zeros(max_depth + 1)\n",
    "sample_sizes = np.zeros(max_depth + 1)\n",
    "\n",
    "for depth in range(max_depth + 1):\n",
    "    # Set depth-wise column names\n",
    "    relevant_nodes_col = f\"relevant_{test_head}_nodes_at_depth_{depth}\"\n",
    "    relevant_map_col = f\"idx_to_node_{test_head}_at_depth_{depth}\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Processing depth {depth} of {max_depth} for {test_head}...\")\n",
    "    depth_dict[test_head] = depth\n",
    "    depth_df, nodes_within_radius = filter_data_with_hops(\n",
    "        output_df, depth_dict, root_graphs, column=\"tgts\"\n",
    "    )\n",
    "    depth_df = depth_df[depth_df[f\"{test_head}_depth_{depth}_applicable\"] == True]\n",
    "    depth_df[\"_masks\"] = depth_df[\"masks\"].apply(\n",
    "        lambda x: np.where(np.array(x) == 0)[0]\n",
    "    )\n",
    "\n",
    "    # Determine which nodes are relevant at current depth after filtering for masks\n",
    "    depth_df[relevant_nodes_col] = depth_df[\"_masks\"].apply(\n",
    "        remove_relevant_masked_nodes, args=(nodes_within_radius,)\n",
    "    )\n",
    "    depth_df[relevant_map_col] = depth_df[relevant_nodes_col].apply(\n",
    "        set_depth_idx_to_text_map, args=(idx_to_node, test_head)\n",
    "    )\n",
    "    depth_df[\"preds\"] = depth_df[\"preds\"].apply(lambda x: np.array(x))\n",
    "    depth_df[\"tgts\"] = depth_df[\"tgts\"].apply(lambda x: np.array(x))\n",
    "\n",
    "    # Filter preds and tgts for relevant nodes\n",
    "    depth_df[\"relevant_preds\"] = depth_df.apply(\n",
    "        extract_relevant_elements, args=(\"preds\", relevant_nodes_col), axis=1\n",
    "    )\n",
    "    depth_df[\"relevant_tgts\"] = depth_df.apply(\n",
    "        extract_relevant_elements, args=(\"tgts\", relevant_nodes_col), axis=1\n",
    "    )\n",
    "\n",
    "    # Have to convert tgts and preds to string to be hashable\n",
    "    depth_df[\"relevant_tgts_str\"] = depth_df[\"relevant_tgts\"].apply(\n",
    "        lambda x: str(list(np.where(x == 1)[0]))\n",
    "    )\n",
    "    depth_df[\"relevant_preds_str\"] = depth_df[\"relevant_preds\"].apply(\n",
    "        lambda x: str(list(np.where(x == 1)[0]))\n",
    "    )\n",
    "\n",
    "    # Get text labels and preds\n",
    "    depth_df[\"text_tgts\"] = depth_df.apply(\n",
    "        translate_idx_to_text_row,\n",
    "        args=(\"relevant_tgts_str\", relevant_map_col, test_head),\n",
    "        axis=1,\n",
    "    )\n",
    "    depth_df[\"text_preds\"] = depth_df.apply(\n",
    "        translate_idx_to_text_row,\n",
    "        args=(\"relevant_preds_str\", relevant_map_col, test_head),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Convert text tgts and preds list to strings\n",
    "    depth_df[\"text_tgts_str\"] = depth_df[\"text_tgts\"].apply(str)\n",
    "    depth_df[\"text_preds_str\"] = depth_df[\"text_preds\"].apply(str)\n",
    "\n",
    "    unique_labels = depth_df[\"text_tgts_str\"].unique()\n",
    "\n",
    "    depth_tp_dict = {}\n",
    "    depth_fp_dict = {}\n",
    "    depth_fn_dict = {}\n",
    "    depth_count_dict = {}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_df = (\n",
    "            depth_df[depth_df[\"text_tgts_str\"] == label].copy().reset_index(drop=True)\n",
    "        )\n",
    "        label_list = label_df[\"text_tgts\"][0]\n",
    "\n",
    "        tp_dict = {}\n",
    "        fp_dict = {}\n",
    "        fn_dict = {}\n",
    "        count_dict = {}\n",
    "\n",
    "        label_df.apply(\n",
    "            decompose_and_score_labels,\n",
    "            args=(tp_dict, fp_dict, fn_dict, count_dict, depth, 0),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        for tgt in tp_dict.keys():\n",
    "            depth_tp_dict[tgt] = depth_tp_dict.get(tgt, 0) + tp_dict[tgt]\n",
    "            depth_fp_dict[tgt] = depth_fp_dict.get(tgt, 0) + fp_dict[tgt]\n",
    "            depth_fn_dict[tgt] = depth_fn_dict.get(tgt, 0) + fn_dict[tgt]\n",
    "            depth_count_dict[tgt] = depth_count_dict.get(tgt, 0) + count_dict[tgt]\n",
    "\n",
    "    depth_precisions = []\n",
    "    depth_recalls = []\n",
    "    depth_f1s = []\n",
    "    depth_counts = []\n",
    "\n",
    "    for tgt in depth_tp_dict.keys():\n",
    "        tgt_precision_denominator = depth_tp_dict[tgt] + depth_fp_dict[tgt]\n",
    "        if tgt_precision_denominator == 0:\n",
    "            tgt_precision = 0\n",
    "        else:\n",
    "            tgt_precision = depth_tp_dict[tgt] / (\n",
    "                depth_tp_dict[tgt] + depth_fp_dict[tgt]\n",
    "            )\n",
    "\n",
    "        tgt_recall_denominator = depth_tp_dict[tgt] + depth_fn_dict[tgt]\n",
    "        if tgt_recall_denominator == 0:\n",
    "            tgt_recall = 0\n",
    "        else:\n",
    "            tgt_recall = depth_tp_dict[tgt] / (depth_tp_dict[tgt] + depth_fn_dict[tgt])\n",
    "\n",
    "        tgt_f1_denominator = tgt_precision + tgt_recall\n",
    "        if tgt_f1_denominator == 0:\n",
    "            tgt_f1 = 0\n",
    "        else:\n",
    "            tgt_f1 = 2 * (tgt_precision * tgt_recall) / (tgt_precision + tgt_recall)\n",
    "\n",
    "        print(\"\\n\\t\" + \"-\" * 40)\n",
    "        print(f\"\\tTarget: {tgt}\")\n",
    "        print(f\"\\tTP: {depth_tp_dict[tgt]}\")\n",
    "        print(f\"\\tFP: {depth_fp_dict[tgt]}\")\n",
    "        print(f\"\\tFN: {depth_fn_dict[tgt]}\")\n",
    "        print(f\"\\tPrecision: {tgt_precision:.2f}\")\n",
    "        print(f\"\\tRecall: {tgt_recall:.2f}\")\n",
    "        print(f\"\\tF1: {tgt_f1:.2f}\")\n",
    "        print(f\"\\tCount: {depth_count_dict[tgt]}\")\n",
    "\n",
    "        depth_precisions.append(tgt_precision)\n",
    "        depth_recalls.append(tgt_recall)\n",
    "        depth_f1s.append(tgt_f1)\n",
    "        depth_counts.append(depth_count_dict[tgt])\n",
    "\n",
    "    depth_precision = np.mean(depth_precisions)\n",
    "    depth_recall = np.mean(depth_recalls)\n",
    "    depth_f1 = np.mean(depth_f1s)\n",
    "    depth_count = np.sum(depth_counts)\n",
    "\n",
    "    print(\"\\n\\t\" + \"-\" * 60)\n",
    "    print(f\"\\tDepth {depth} precision: {depth_precision:.2f}\")\n",
    "    print(f\"\\tDepth {depth} recall: {depth_recall:.2f}\")\n",
    "    print(f\"\\tDepth {depth} F1: {depth_f1:.2f}\")\n",
    "    print(f\"\\tDepth {depth} count: {depth_count}\")\n",
    "\n",
    "    precision[depth] = depth_precision\n",
    "    recall[depth] = depth_recall\n",
    "    f1[depth] = depth_f1\n",
    "    sample_sizes[depth] = depth_count\n",
    "\n",
    "head_precision = np.sum(precision * sample_sizes) / np.sum(sample_sizes)\n",
    "head_recall = np.sum(recall * sample_sizes) / np.sum(sample_sizes)\n",
    "head_f1 = np.sum(f1 * sample_sizes) / np.sum(sample_sizes)\n",
    "macro_head_precision = np.mean(precision)\n",
    "macro_head_recall = np.mean(recall)\n",
    "macro_head_f1 = np.mean(f1)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\n",
    "    f\"Precision for {test_head}, sample-weighted: {head_precision:.2f}, macro: {macro_head_precision:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Recall for {test_head}, sample-weighted: {head_recall:.2f}, macro: {macro_head_recall:.2f}\"\n",
    ")\n",
    "print(f\"F1 for {test_head}, sample-weighted: {head_f1:.2f}, macro: {macro_head_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(G=root_graphs[\"substrate\"], size=5, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
